{
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.21",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "demo",
    "createdOn": 1761572597257,
    "customFields": {},
    "tags": [],
    "creator": "admin"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate automatic documentation for datasets, projects, and flow zones\n\nWritten by Tim Honker - Oct 2025\n\n```Warning: this notebook will fully overwrite the entirety of dataset descriptions if they are not already 100% filled out```\n\nThis notebook will iterate through all projects on the local DSS node (or a user specified list) and:\n\n1. Check each dataset to see if it has ALL of the following: short description, long description, and each column has a description. If ANY one of these is empty, then it will re-generate ALL of them using AI and automatically save them.\n \n2. Fill in any empty Flow Zone descriptions (if either the short or long or both are empty)\n\n3. Fill in any empty Project descriptions. (Short descriptions are ignored).\n\nThis is code to prototype a future DSS Plugin that I\u0027m working on with Abi Edwards.\n\n\nPossible future features:\n\n* add support for tagging entities (Projects, Datasets, Flow Zones) where the description was AI generated\n* add support for a special tag that prevents any changes to entities, so projects, etc can be OPT\u0027ed out of AI generated descriptions\n* more difficult - add support for only filling in empty fields in datasets and don\u0027t change previous occupied ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup / Config"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from json import JSONDecodeError, dumps\nimport sys\nimport traceback\n\nimport pandas as pd\nimport dataiku\nfrom dataikuapi.utils import DataikuException\nimport dataikuapi"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Future plugin level settings:\n\n# The list of Projects to affect. If not defined, then it will default to ALL projects\n# PROJECTS_LIST \u003d [\u0027SOL_MBA\u0027, \u0027HCP_TARGET_AGENTS\u0027, \u0027GOOGLE_DRIVE_AUTOMATIC_TRIGGER\u0027]\n\n# Set SAVE_DESCRIPTION to False for a dry-run, where it will generate the descriptions, but not save them.\n# Set to True to save the AI-generated descriptions.\nSAVE_DESCRIPTION \u003d True\n\n# Language to generate descriptions in. \n# Supported languages are “dutch”, “english”, “french”, “german”, “portuguese”, and “spanish”\nLANGUAGE \u003d \u0027english\u0027\n\n################################################################################\n# Settings for generating descriptions for Projects\n################################################################################\n#    https://developer.dataiku.com/latest/api-reference/python/projects.html#dataikuapi.dss.project.DSSProject.generate_ai_description\n# The purpose of the generated description. \n# Supported purposes are “generic”, “technical”, “business_oriented”, and “executive” (defaults to generic).\nPROJECT_PURPOSE\u003d\u0027generic\u0027\n\n# The length of the generated description. \n# Supported lengths are “low”, “medium”, and “high” (defaults to medium).\nPROJECT_LENGTH\u003d\u0027medium\u0027\n\n################################################################################\n# Settings for generating descriptions for Flow Zones\n################################################################################\n# same as Projects\n#   https://developer.dataiku.com/latest/api-reference/python/flow.html#dataikuapi.dss.flow.DSSFlowZone.generate_ai_description\nFLOWZONE_PURPOSE\u003d\u0027generic\u0027\n\nFLOWZONE_LENGTH\u003d\u0027medium\u0027"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "client \u003d dataiku.api_client()\n\n# if it is not defined, then default to all projects on the local node\ntry:\n    PROJECTS_LIST\nexcept NameError:\n    PROJECTS_LIST \u003d client.list_project_keys()\n\n# only reset to zero if undefined - useful for keeping track of the number of AI services called\n# in a single 24 hour period - assuming you don\u0027t restart this Jupyter Notebook\u0027s kernel.\ntry:\n    num_AI_services_used\nexcept NameError:\n    num_AI_services_used \u003d 0"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function definitions"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def is_project_empty(project_handle):\n    \"\"\"This traps the JSONDecodeError exception that occurs when generate_ai_description is called\n    if the project has no datasets or recipes or flow zones\"\"\"\n\n    # Check if there are any datasets\n    has_datasets \u003d len(project_handle.list_datasets()) \u003e 0\n\n    # Check if there are any recipes\n    has_recipes \u003d len(project_handle.list_recipes()) \u003e 0\n\n    return (not has_datasets) and (not has_recipes)\n\n\ndef get_dataset_long_description(dataset_handle):\n    dataset_metadata \u003d dataset_handle.get_metadata()\n    try:\n        return dataset_metadata[\u0027description\u0027]\n    except KeyError:\n        return \u0027\u0027\n\n\ndef get_dataset_short_description(dataset_handle):\n    dataset_settings \u003d dataset_handle.get_settings().get_raw()\n    try:\n        return dataset_settings[\u0027shortDesc\u0027]\n    except KeyError:\n        return \u0027\u0027\n\n\ndef get_dataset_column_descriptions(dataset_handle):\n    dataset_schema \u003d dataset_handle.get_schema()\n    try:\n        return [item[\"comment\"] for item in dataset_schema[\u0027columns\u0027]]\n    except KeyError:\n        return \u0027\u0027\n\n\ndef dataset_has_full_documentation(project_handle, dataset_id):\n    \"\"\"\n    Returns a boolean describing if a specific dataset meets ALL of the following requirements:\n        1) It has a shortDesc that is not empty\n        2) It has a description that is not empty\n        3) ALL columns have descriptions that are not empty.\n\n    Useful for determining if we should auto-generate a description to fill it in.\n    \"\"\"\n\n    # project_handle \u003d client.get_project(project_key)\n    dataset_handle \u003d project_handle.get_dataset(dataset_id)\n    \n    if not get_dataset_long_description(dataset_handle):\n        # print(f\u0027Dataset {dataset_id} lacks full documentation because empty: Long Description\u0027)\n        return False\n    \n    if not get_dataset_short_description(dataset_handle):\n        # print(f\u0027Dataset {dataset_id} lacks full documentation because empty: Short Description\u0027)\n        return False\n    \n    column_descriptions \u003d get_dataset_column_descriptions(dataset_handle)\n\n    if any(not s or not s.strip() for s in column_descriptions):\n        # print(f\u0027Dataset {dataset_id} lacks full documentation because empty: Column descriptions\u0027)\n        return False\n    \n    # print(f\u0027Dataset {dataset_id} has all description fields filled out.\u0027)\n    return True\n\n\ndef is_project_description_empty(project_handle):\n    \"\"\"\n    Returns a boolean describing if a specific project an existing description.\n    Useful for determining if we should auto-generate a description to fill it in.\n    \"\"\"\n\n    try:\n        md \u003d project_handle.get_metadata()\n        if not md[\u0027description\u0027]:\n            return True\n    except KeyError:\n        return True\n    return False\n\n\ndef pretty_print_dict(d):\n    import json\n    print(json.dump(d, fp\u003dsys.stdout, indent\u003d4, sort_keys\u003dTrue))\n\n\ndef flow_zone_has_description(flow_zone_handle):\n    \"\"\"\n    Returns a boolean describing if a specific flow zone has an existing description.\n    Useful for determining if we should auto-generate a description to fill it in.\n    \"\"\"\n    try:\n        # the auto-generated ones ONLY CREATE THE LONG DESCRIPTION, don\u0027t do the short one.\n        flow_zone_settings                   \u003d flow_zone_handle.get_settings().get_raw()\n        flow_zone_description                \u003d flow_zone_settings.get(\u0027description\u0027, \u0027\u0027)\n        flow_zone_description_length         \u003d len(flow_zone_description)\n\n        # can\u0027t use the \"not \" version, must use len\u003e0 for some reason.\n        flow_zone_has_a_nonempty_description \u003d len(flow_zone_description) \u003e 0 \n\n        return flow_zone_has_a_nonempty_description\n    except JSONDecodeError:\n        print(f\"[ERROR] Trying to get description for {flow_zone_settings[\u0027name\u0027]}\")\n        pretty_print_dict(flow_zone_settings)\n        return False\n    \n    \ndef is_flowzone_empty(flowzone_handle):\n    \"\"\"This traps the JSONDecodeError exception that occurs when generate_ai_description is called\n    if the flowzone has no datasets or recipes\n    \n      There has to be at least one recipe or one dataset in order to explain a zone.\n\n    \"\"\"\n    for i in flowzone_handle.items:\n        if type(i) in [dataiku.Dataset, dataikuapi.dss.recipe.DSSRecipe, dataikuapi.dss.dataset.DSSDataset]:\n            return False\n    return True\n\n\ndef read_first_dataset_row(project_key, dataset_name):\n    \"\"\"Returns a boolean if the first row of the dataset was readable.\n    False implies the dataset was either empty or an exception occurred.\n    Useful for determining if autogenerated descriptions of a dataset are possible.\n    \"\"\"\n    try:\n        dataset_handle \u003d dataiku.Dataset(dataset_name, project_key\u003dproject_key)\n        df \u003d dataset_handle.get_dataframe(limit\u003d1)\n\n        # Check if the dataframe is empty (no rows)\n        if df.empty:\n            print(\"Dataset is empty.\")\n            return False\n\n        return True\n\n    except Exception as e:\n        # Catch any exception that might occur\n        return False\n\n\n# def tag_object_as_auto_generated(object_handle):\n#     \"\"\"\n#     x\n#     \"\"\"\n#     if isinstance(object_handle, dataiku.Dataset):\n        \n#     elif isinstance(object_handle, dataiku.DSSProject):\n#         project_tags \u003d project.get_tags()\n#         project_tags.append(\"AI_generated_description\")\n#         project.set_tags(project_tags)\n# #     elif isinstance(object_handle, dataiku.Dataset):\n\n        \n\n        \n\n#         dataset_settings \u003d dataset.get_settings()\n#         dataset_settings.tags.append(\"AI_generated_description\")\n#         dataset_settings.save()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Datasets"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "# iterate through projects\nfor project_key in PROJECTS_LIST:\n    project_handle \u003d client.get_project(project_key)\n\n    # iterate through all datasets in that project\n    for dataset in project_handle.list_datasets():\n        dataset_id \u003d dataset[\u0027name\u0027]\n        dataset_handle \u003d project_handle.get_dataset(dataset_id)\n\n        if not dataset_handle.exists():\n            print(f\"[SKIP] dataset does not exist:   {project_key} - {dataset_id}\")\n            continue\n            \n        # check if there is no schema\n        if len(dataset[\u0027schema\u0027].get(\u0027columns\u0027,\u0027\u0027)) \u003d\u003d 0:\n            print(f\"[SKIP] dataset has empty schema: {project_key} - {dataset_id}\")\n            continue\n    \n        # skip this dataset if it already has all of the description fields filled out\n        if not dataset_has_full_documentation(project_handle, dataset_id):\n            \n            # test if the first row can be read. VERY IMPORTANT to filter out a lot of \n            # wasted AI Services calls.\n            if not read_first_dataset_row(project_key, dataset_id):\n                print(f\"[SKIP] dataset could not be read: {project_key} - {dataset_id}\")\n                continue\n            \n            print(f\"Auto-generating documentation for {project_key}\u0027s dataset: {dataset_id} ...\")\n            try:\n                # always increment this BEFORE calling generate_ai_description since generate_ai_description\n                # often raises an exception\n                num_AI_services_used +\u003d 1\n                \n                # this blocks execution, doesn\u0027t utilize Futures/JobID system\n                # actually generate and save the description\n                _ \u003d dataset_handle.generate_ai_description(language\u003dLANGUAGE, save_description\u003dSAVE_DESCRIPTION)\n                \n#                 if SAVE_DESCRIPTION and dataset_has_full_documentation(project_handle, dataset_id):\n#                     print(f\"Successfully filled out all fields for {dataset_id}\")\n#                 else:\n#                     print(f\"Attempted to fill in description for dataset, but failed to take effect: {dataset_id}\")\n#                     print(x)\n                \n            except DataikuException as e:\n                # there are so many different types of exceptions that occur\n                # java.lang.IllegalArgumentException: Column not found in schema:\n                print(f\"[ERROR] Exception {e} when autofilling:         {project_key} - {dataset_id}\")\n                continue"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flow Zones"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Iterate through the list of projects\nfor project_key in PROJECTS_LIST:\n    project_handle \u003d client.get_project(project_key)\n    flow_handle \u003d project_handle.get_flow()\n    \n    # Iterate through each flow zone in a specific project\n    for flow_zone_handle in flow_handle.list_zones():\n        try:\n            # Ensure that the flow zone meets the requirements for AI-Gen descriptions before\n            # attempting to have AI generate the description.\n            if is_flowzone_empty(flow_zone_handle):\n                print(f\"[SKIP] Flow zone must have dataset or recipe in it to autogenerate description: {project_key} - {flow_zone_name}\")\n                continue\n\n            # get the settings and name of the flow zone\n            flow_zone_settings \u003d flow_zone_handle.get_settings().get_raw()\n            flow_zone_name \u003d flow_zone_settings.get(\u0027name\u0027,\u0027\u0027)\n            \n            # only have AI write the description if there is not one there already\n            if not(flow_zone_has_description(flow_zone_handle)):\n                print(f\"[CREATE] Generating flow zone documentation for {project_key} - {flow_zone_name}\")\n                num_AI_services_used +\u003d 1\n                flow_zone_handle.generate_ai_description(\n                    language\u003dLANGUAGE,\n                    purpose\u003dFLOWZONE_PURPOSE,\n                    length\u003dFLOWZONE_LENGTH,\n                    save_description\u003dSAVE_DESCRIPTION\n                )\n#             else:\n#                 print(f\"[SKIP] Flow zone already has a description: {project_key} - {flow_zone_name}\")\n        except (DataikuException, JSONDecodeError) as e:\n            print(f\"[ERROR] Creating flow zone description for {project_key} - {flow_zone_name}\")\n            pretty_print_dict(flow_zone_settings)\n            continue"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Projects"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# iterate through the list of projects\nfor project_key in PROJECTS_LIST:\n    try:\n        project_handle \u003d client.get_project(project_key)\n        \n        # Ensure that the project meets the requirements for creating AI generated descriptions\n        if is_project_empty(project_handle):\n            print(f\"[SKIP] Project must have datasets or recipes in flow, can\u0027t create description: {project_key}\")\n            continue\n\n        # Only generate descriptions if there is not one already:\n        if is_project_description_empty(project_handle):\n            print(f\"Project {project_key} has an empty description, generating AI description for it.\")\n            num_AI_services_used +\u003d 1\n\n            # https://developer.dataiku.com/latest/api-reference/python/projects.html#dataikuapi.dss.project.DSSProject.generate_ai_description\n            project_handle.generate_ai_description(\n                language\u003dLANGUAGE,\n                purpose\u003dPROJECT_PURPOSE,\n                length\u003dPROJECT_LENGTH,\n                save_description\u003dSAVE_DESCRIPTION\n            )\n\n    except JSONDecodeError:\n        print(f\"[JSONDecodeError] Creating project description for {project_key}\")\n        continue"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display the number of AI Services called.\nprint(f\"Successfully finished. Used {num_AI_services_used} calls to Dataiku\u0027s API Services which are limited to a total of 1000/day\")"
      ],
      "outputs": []
    }
  ]
}