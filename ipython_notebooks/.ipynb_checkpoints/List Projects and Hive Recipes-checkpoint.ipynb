{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.21",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "creator": "demo",
    "modifiedBy": "demo",
    "createdOn": 1756999673948,
    "customFields": {},
    "tags": []
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name \u003d \"Dataiku_Hive_Project_Migration\"\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datetime import datetime\n\nimport dataiku\nimport pandas as pd\nimport pprint\n\nfrom io import StringIO\nfrom IPython.display import FileLink\n\n# Connect to the DSS instance\nclient \u003d dataiku.api_client()\nproject \u003d client.get_default_project()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "existing_datasets \u003d [ds[\"name\"] for ds in project.list_datasets()]\n\nif dataset_name in existing_datasets:\n    print(f\"✅ Dataset \u0027{dataset_name}\u0027 already exists.\")\nelse:\n    print(f\"⚠️ Dataset \u0027{dataset_name}\u0027 not found. Creating it now...\")\n\n    # Step 2: Define dataset params\n    # This example creates a filesystem (CSV) dataset in the \u0027filesystem_managed\u0027 connection\n    project.create_dataset(\n        dataset_name\u003ddataset_name,\n        type\u003d\"Filesystem\",\n        params\u003d{\n            \"connection\": \"filesystem_managed\",   # Make sure this connection exists in your DSS\n            \"path\": dataset_name,                 # Folder name \u003d dataset name\n            \"format\": \"csv\"\n        },\n        formatType\u003d\"csv\"\n    )\n\n    print(f\"✅ Dataset \u0027{dataset_name}\u0027 has been created.\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare a list to store results\nproject_hive_counts \u003d []\n\n# Iterate over all projects\nprojects \u003d client.list_project_keys()\nfor project_key in projects:\n    project \u003d client.get_project(project_key)\n    \n    # Get the list of recipes in the project\n    recipes \u003d project.list_recipes()\n    \n    # Count the number of Hive recipes\n    hive_count \u003d sum(1 for recipe in recipes if recipe[\u0027type\u0027] \u003d\u003d \u0027Hive\u0027)\n\n    # Append the result to the list\n    project_hive_counts.append({\u0027project_key\u0027: project_key, \u0027hive_recipe_count\u0027: hive_count})\n\n# Create a Pandas DataFrame with the results\ndf \u003d pd.DataFrame(project_hive_counts)\ndf.sort_values(by\u003d\"hive_recipe_count\", ascending\u003dFalse)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n\n# Connect to the DSS instance\nclient \u003d dataiku.api_client()\n\n# Prepare a list to store results\nproject_info \u003d []\n\n# Iterate over all projects\nprojects \u003d client.list_project_keys()\nfor project_key in projects:\n    try:\n        project \u003d client.get_project(project_key)\n        \n        # Get project metadata to extract owner information\n        project_metadata \u003d project.get_metadata()\n        project_summary \u003d project.get_summary()\n        last_modified_timestamp \u003d project_summary.get(\u0027versionTag\u0027, None).get(\u0027lastModifiedOn\u0027, None)\n        if last_modified_timestamp:\n            last_modified_date \u003d datetime.fromtimestamp(last_modified_timestamp / 1000).strftime(\u0027%Y-%m-%d %H:%M:%S\u0027)\n        else:\n            last_modified_date \u003d \u0027unknown_date\u0027\n        \n        # Get the owner username\n        #owner_username \u003d project_metadata.get(\u0027owner\u0027, \u0027\u0027)\n        \n        # Get the list of recipes in the project\n        recipes \u003d project.list_recipes()\n        \n        # Count the number of Hive recipes\n        hive_recipes \u003d [recipe for recipe in recipes if recipe[\u0027type\u0027].lower() \u003d\u003d \u0027hive\u0027]\n        hive_count \u003d len(hive_recipes)\n        \n        dataset_types \u003d set()\n        \n        for d in  project.list_datasets():\n#            if d.get(\u0027type\u0027) \u003d\u003d \u0027hive\u0027:\n                dataset_types.add(d.get(\u0027type\u0027))\n        \n        # Append the result to the list\n        project_info.append({\n            \u0027PROJECT_KEY\u0027: project_key, \n            \u0027NUMBER_OF_HIVE_RECIPES_USED_IN_THIS_PROJECT\u0027: hive_count,\n            \u0027DATASET_TYPES\u0027: dataset_types,\n            \u0027PERMISSIONS\u0027: project.get_permissions(),\n            #\u0027PROJECT_OWNER_EMAIL\u0027: owner_email,\n            \u0027DATE_PROJECT_WAS_LAST_MODIFIED\u0027: last_modified_date,\n            # \u0027DATE_LAST_JOB_WAS_RUN\u0027: last_job_date\n        })\n    except Exception as e:\n        print(f\"Error processing project {project_key}: {str(e)}\")\n        # Add error entry to maintain record of all projects\n        project_info.append({\n            \u0027PROJECT_KEY\u0027: project_key,\n            \u0027NUMBER_OF_HIVE_RECIPES_USED_IN_THIS_PROJECT\u0027: -1,  # Error indicator\n            #\u0027PROJECT_OWNER_USERNAME\u0027: \"\",\n            #\u0027PROJECT_OWNER_EMAIL\u0027: \"\",\n            \u0027DATE_PROJECT_WAS_LAST_MODIFIED\u0027: \"unknown\",\n            # \u0027DATE_LAST_JOB_WAS_RUN\u0027: \"\",\n        })\n\n# Create a Pandas DataFrame with the results\ndf \u003d pd.DataFrame(project_info)\ndf[\u0027PERMISSIONS_EXPANDED\u0027] \u003d df[\u0027PERMISSIONS\u0027].apply(lambda d: pprint.pformat(d, width\u003d1000))\ndf"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(df)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# import os\n# os.getcwd()\n\n# output_filename \u003d \"downloads/Dataiku_Hive_Project_Migration.csv\"\n# os.makedirs(\"downloads\", exist_ok\u003dTrue)\n# df.to_csv(output_filename, index\u003dFalse)\n\n# FileLink(os.path.join(os.getcwd(),output_filename))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "output_dataset \u003d dataiku.Dataset(\"Dataiku_Hive_Project_Migration\")\noutput_dataset.write_with_schema(df)"
      ],
      "outputs": []
    }
  ]
}