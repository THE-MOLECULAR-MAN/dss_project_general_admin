{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.21",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "createdOn": 1764095416956,
    "customFields": {},
    "creator": "demo",
    "tags": [],
    "modifiedBy": "demo"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nimport pandas as pd\nimport time\nimport json\n\ndef analyze_agent_costs_final(audit_path\u003d\"/data/dataiku/dss_data/run/audit\"):\n    client \u003d dataiku.api_client()\n    project \u003d client.get_default_project()\n    \n    # Unique names for temp objects\n    ts \u003d int(time.time())\n    conn_name \u003d f\"tmp_audit_conn_{ts}\"\n    ds_name \u003d f\"tmp_audit_logs_{ts}\"\n    \n    dataset \u003d None\n    conn \u003d None\n\n    print(f\"1. Creating temporary connection to: {audit_path}\")\n    try:\n        # Create a Filesystem connection rooted at the audit log path\n        # The DSS Backend (running as \u0027dataiku\u0027) will access this path\n        conn \u003d client.create_connection(conn_name, \"Filesystem\", {\n            \"root\": audit_path\n        })\n    except Exception as e:\n        print(f\"Error creating connection. Ensure you have Admin rights. {e}\")\n        return None\n\n    try:\n        print(f\"2. Creating temporary dataset: {ds_name} (Raw Line Mode)\")\n        # We use \u0027csv\u0027 because it is the only format that supports raw line reading via separator hacks\n        dataset \u003d project.create_dataset(ds_name, \"Filesystem\", params\u003d{\n            \"connection\": conn_name,\n            \"path\": \"/\" # Root of the connection\n        }, formatType\u003d\"csv\")\n        \n        # Configure CSV to act like a \"Line Reader\"\n        # We use a separator that doesn\u0027t exist in the file (\\x1F) so the whole line becomes Column 0\n        settings \u003d dataset.get_settings()\n        settings.get_raw()[\"formatParams\"] \u003d {\n            \"style\": \"excel\",\n            \"separator\": \"\\x1F\",  # Unit Separator (ASCII 31) - highly unlikely in JSON\n            \"quoteChar\": \"\",      # Disable quoting to prevent \u0027unexpected EOF\u0027 on JSON quotes\n            \"escapeChar\": \"\",     # Disable escaping to read literal chars\n            \"parseHeaderRow\": False,\n            \"skipRowsBeforeHeader\": 0,\n            \"charset\": \"utf8\"\n        }\n        \n        # Force a single-column schema\n        settings.get_raw()[\"schema\"] \u003d {\n            \"columns\": [{\"name\": \"line\", \"type\": \"string\"}]\n        }\n        settings.save()\n\n        print(\"3. Streaming and parsing log lines...\")\n        \n        stats \u003d {}\n        # We use the internal dataiku.Dataset to read efficiently\n        dku_ds \u003d dataiku.Dataset(ds_name)\n        \n        # Iterate through the file line by line (in chunks)\n        print(\"OUTER LOOP starting...\")\n       \n        for df in dku_ds.iter_dataframes(chunksize\u003d5000):\n            # Dynamic column finding (in case DSS names it \u0027col0\u0027 instead of \u0027line\u0027)\n            try:\n                # print(\"OUTER LOOP TOP\")\n                col_name \u003d df.columns[0]\n\n                # Iterate over rows in the dataframe\n                for raw_line in df[col_name]:\n                    try:\n                        # print(\"INNER LOOP TOP\")                        \n                        if not isinstance(raw_line, str):\n                            continue\n\n                        # 3a. Parse JSON manually\n                        try:\n                            event \u003d json.loads(raw_line)\n                        # except json.JSONDecodeError:\n                        except Exception as e:\n                            # This catches the \"dirty\" lines (stack traces, etc.) and skips them\n                            print(f\"Exception - inner loop - MID, continuing {e}\")\n\n                            continue\n\n                        # 3b. Filter for LLM events\n                        # \u0027topic\u0027 usually identifies the event type\n                        topic \u003d event.get(\u0027topic\u0027, \u0027\u0027)\n                        if \u0027llm\u0027 not in topic and \u0027external-model\u0027 not in topic:\n                            continue\n\n                        print(\"INNER LOOP 20\")\n                        # 3c. Extract Data\n                        data \u003d event.get(\u0027data\u0027, {})\n\n                        # Optional: Skip failed calls (remove if you want to count errors)\n                        if data.get(\u0027outcome\u0027) !\u003d \u0027SUCCESS\u0027:\n                            continue\n\n                        details \u003d data.get(\u0027details\u0027, {})\n                        usage \u003d data.get(\u0027usage\u0027, {})\n                        context \u003d data.get(\u0027context\u0027, {})\n                        target \u003d data.get(\u0027target\u0027, {})\n\n                        # 3d. Identify Agent \u0026 Model\n                        # Heuristic: Check Context -\u003e Details -\u003e Target\n                        agent \u003d context.get(\u0027agentName\u0027) or \\\n                                details.get(\u0027agentName\u0027) or \\\n                                context.get(\u0027agentId\u0027) or \\\n                                \"Direct/Unknown\"\n\n                        model \u003d details.get(\u0027llmId\u0027) or \\\n                                target.get(\u0027llmId\u0027) or \\\n                                \"N/A\"\n\n                        # 3e. Extract Metrics\n                        cost \u003d usage.get(\u0027estimatedCost\u0027, 0.0)\n                        tokens \u003d usage.get(\u0027totalTokens\u0027, 0)\n\n                        # 3f. Aggregate\n                        key \u003d (agent, model)\n                        if key not in stats:\n                            stats[key] \u003d {\u0027cost\u0027: 0.0, \u0027tokens\u0027: 0, \u0027calls\u0027: 0}\n\n                        stats[key][\u0027cost\u0027] +\u003d float(cost or 0)\n                        stats[key][\u0027tokens\u0027] +\u003d int(tokens or 0)\n                        stats[key][\u0027calls\u0027] +\u003d 1\n                        print(\"INNER LOOP BOTTOM - SAFE\")\n                    except Exception as e:\n                        print(f\"Exception - inner loop, continuing {e}\")\n                        continue\n            except Exception as e:\n                print(f\"Exception - outer loop, continuing {e}\")\n                continue\n            print(\"ZZZZZZ\")\n        print(\"Outer loop successfully finished.\")\n\n        # 4. Format Output\n        print(\"FORMATTING OUTPUT...\")\n        results \u003d []\n        for (agent, model), metrics in stats.items():\n            results.append({\n                \"Agent Name\": agent,\n                \"LLM Model\": model,\n                \"Total Cost ($)\": round(metrics[\u0027cost\u0027], 4),\n                \"Total Tokens\": int(metrics[\u0027tokens\u0027]),\n                \"Call Count\": int(metrics[\u0027calls\u0027])\n            })\n            \n        if not results:\n            return pd.DataFrame(columns\u003d[\"Status\"], data\u003d[\"No LLM usage events found in logs\"]) \n            \n        return pd.DataFrame(results).sort_values(\"Total Cost ($)\", ascending\u003dFalse)\n\n    except Exception as e:\n        print(f\"\\nCRITICAL ERROR: {e}\")\n        import traceback\n        traceback.print_exc()\n        #return None\n\n    finally:\n        print(\"4. Cleaning up temporary artifacts...\")\n        if dataset:\n            #return pd.DataFrame(results)\n            try: dataset.delete()\n            except: pass\n        if conn:\n            try: conn.delete()\n            except: pass\n        \n\n# --- Execution ---\nLOG_PATH \u003d \"/data/dataiku/dss_data/run/audit\" \ndf_report \u003d analyze_agent_costs_final(LOG_PATH)\n\nif df_report is not None:\n    print(\"\\n--- Agent Cost \u0026 Utilization Report ---\")\n    print(df_report.to_string(index\u003dFalse))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_report"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nimport pandas as pd\nimport time\nimport json\nimport os\n\ndef analyze_agent_costs_parent_conn(audit_path\u003d\"/data/dataiku/dss_data/run/audit\"):\n    client \u003d dataiku.api_client()\n    project \u003d client.get_default_project()\n    \n    # Calculate Parent Directory and Target Folder Name\n    parent_dir \u003d os.path.dirname(audit_path.rstrip(\u0027/\u0027))\n    target_subfolder \u003d \"/\" + os.path.basename(audit_path.rstrip(\u0027/\u0027))\n    \n    ts \u003d int(time.time())\n    conn_name \u003d f\"tmp_audit_root_{ts}\"\n    folder_name \u003d f\"tmp_audit_access_{ts}\"\n    \n    conn \u003d None\n    folder \u003d None\n    \n    print(f\"1. Creating temporary connection to PARENT: {parent_dir}\")\n    try:\n        conn \u003d client.create_connection(conn_name, \"Filesystem\", {\n            \"root\": parent_dir\n        })\n    except Exception as e:\n        print(f\"Error creating connection: {e}\")\n        return None\n\n    try:\n        print(f\"2. Creating temporary Folder pointing to: {target_subfolder}\")\n        folder \u003d project.create_managed_folder(folder_name)\n        settings \u003d folder.get_settings()\n        settings.get_raw()[\"type\"] \u003d \"Filesystem\"\n        settings.get_raw()[\"params\"] \u003d {\n            \"connection\": conn_name, \n            \"path\": target_subfolder \n        }\n        settings.save()\n        \n        print(\"3. Listing and streaming log files...\")\n        dku_folder \u003d dataiku.Folder(folder_name)\n        \n        try:\n            all_paths \u003d dku_folder.list_paths_in_partition()\n        except Exception as e:\n            print(f\"   Error listing files: {e}\")\n            return None\n\n        print(f\"   Found {len(all_paths)} files.\")\n        \n        stats \u003d {}\n        \n        for file_path in all_paths:\n            # Only process audit logs\n            if \"audit.log\" not in file_path:\n                continue\n                \n            print(f\"   Processing: {file_path} ...\")\n            \n            try:\n                # Get raw stream (File-like object)\n                with dku_folder.get_download_stream(file_path) as stream:\n                    \n                    # --- FIX: Iterate directly over the stream object ---\n                    for line_bytes in stream:\n                        if not line_bytes: continue\n                            \n                        try:\n                            # Robust decoding\n                            line_str \u003d line_bytes.decode(\u0027utf-8\u0027, errors\u003d\u0027replace\u0027).strip()\n                            if not line_str: continue\n                            \n                            event \u003d json.loads(line_str)\n                            \n                            # --- Filter \u0026 Extraction ---\n                            topic \u003d event.get(\u0027topic\u0027, \u0027\u0027)\n                            if \u0027llm\u0027 not in topic and \u0027external-model\u0027 not in topic:\n                                continue\n                            \n                            data \u003d event.get(\u0027data\u0027, {})\n                            if data.get(\u0027outcome\u0027) !\u003d \u0027SUCCESS\u0027:\n                                continue\n\n                            details \u003d data.get(\u0027details\u0027, {})\n                            usage \u003d data.get(\u0027usage\u0027, {})\n                            context \u003d data.get(\u0027context\u0027, {})\n                            target \u003d data.get(\u0027target\u0027, {})\n\n                            # Identity Heuristics\n                            agent \u003d context.get(\u0027agentName\u0027) or details.get(\u0027agentName\u0027) or context.get(\u0027agentId\u0027) or \"Direct/Unknown\"\n                            model \u003d details.get(\u0027llmId\u0027) or target.get(\u0027llmId\u0027) or \"N/A\"\n                            \n                            # Metrics\n                            cost \u003d usage.get(\u0027estimatedCost\u0027, 0.0)\n                            tokens \u003d usage.get(\u0027totalTokens\u0027, 0)\n                            \n                            # Aggregate\n                            key \u003d (agent, model)\n                            if key not in stats:\n                                stats[key] \u003d {\u0027cost\u0027: 0.0, \u0027tokens\u0027: 0, \u0027calls\u0027: 0}\n                            \n                            stats[key][\u0027cost\u0027] +\u003d float(cost or 0)\n                            stats[key][\u0027tokens\u0027] +\u003d int(tokens or 0)\n                            stats[key][\u0027calls\u0027] +\u003d 1\n                            \n                        except (json.JSONDecodeError, ValueError):\n                            continue \n                            \n            except Exception as e:\n                print(f\"   WARNING: Skipped file {file_path}: {e}\")\n                continue\n\n        # 4. Format Output\n        results \u003d []\n        for (agent, model), metrics in stats.items():\n            results.append({\n                \"Agent Name\": agent,\n                \"LLM Model\": model,\n                \"Total Cost ($)\": round(metrics[\u0027cost\u0027], 4),\n                \"Total Tokens\": int(metrics[\u0027tokens\u0027]),\n                \"Call Count\": int(metrics[\u0027calls\u0027])\n            })\n            \n        if not results:\n            return pd.DataFrame(columns\u003d[\"Status\"], data\u003d[\"No LLM usage events found in logs\"]) \n            \n        return pd.DataFrame(results).sort_values(\"Total Cost ($)\", ascending\u003dFalse)\n\n    except Exception as e:\n        print(f\"\\nCRITICAL ERROR: {e}\")\n        return None\n\n    finally:\n        print(\"4. Cleaning up temporary artifacts...\")\n        if folder:\n            try: folder.delete()\n            except: pass\n        if conn:\n            try: conn.delete()\n            except: pass\n\n# --- Execution ---\nLOG_PATH \u003d \"/data/dataiku/dss_data/run/audit\" \n\ndf_report \u003d analyze_agent_costs_parent_conn(LOG_PATH)\n\nif df_report is not None:\n    print(\"\\n--- Agent Cost \u0026 Utilization Report ---\")\n    print(df_report.to_string(index\u003dFalse))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}