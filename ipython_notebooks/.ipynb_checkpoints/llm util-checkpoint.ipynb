{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.21",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "createdOn": 1764095416956,
    "customFields": {},
    "creator": "demo",
    "tags": [],
    "modifiedBy": "demo"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nimport pandas as pd\nimport time\nimport json\n\ndef analyze_agent_costs_via_connection(audit_path\u003d\"/data/dataiku/dss_data/run/audit\"):\n    client \u003d dataiku.api_client()\n    project \u003d client.get_default_project()\n    \n    # Unique names for temp objects\n    # We use a timestamp to ensure we don\u0027t conflict with existing items\n    ts \u003d int(time.time())\n    conn_name \u003d f\"tmp_audit_conn_{ts}\"\n    ds_name \u003d f\"tmp_audit_logs_{ts}\"\n    \n    print(f\"1. Creating temporary connection to: {audit_path}\")\n    try:\n        # Create a Filesystem connection rooted at the audit log path\n        # The DSS Backend (running as \u0027dataiku\u0027) will access this path\n        conn \u003d client.create_connection(conn_name, \"Filesystem\", {\n            \"root\": audit_path\n        })\n    except Exception as e:\n        print(f\"Error creating connection. Ensure you have Admin rights. {e}\")\n        return None\n\n    try:\n        print(f\"2. Creating temporary dataset: {ds_name}\")\n        # Create a dataset pointing to the logs\n        dataset \u003d project.create_dataset(ds_name, \"Filesystem\", params\u003d{\n            \"connection\": conn_name,\n            \"path\": \"/\" # Root of the connection\n        }, formatType\u003d\"json\")\n        \n        # Configure format to handle \"One JSON object per line\"\n        # This matches the structure of audit.log files\n        settings \u003d dataset.get_settings()\n        settings.get_raw()[\"formatParams\"] \u003d {\n            \"style\": \"no_array\",     # One object per line\n            \"charset\": \"utf8\",\n            \"ignoreBadRecords\": True # crucial for log rotation headers/footers\n        }\n        settings.save()\n\n        print(\"3. Reading and aggregating data (this may take a moment)...\")\n        \n        # We iterate in chunks to avoid OOM on large log histories\n        stats \u003d {}\n        \n        # We use the internal dataiku.Dataset to read efficiently\n        # This uses the Backend to stream data, bypassing OS permissions\n        dku_ds \u003d dataiku.Dataset(ds_name)\n        \n        for df in dku_ds.iter_dataframes(chunksize\u003d10000):\n            # Filter for LLM topics immediately\n            if \u0027topic\u0027 in df.columns:\n                # Keep only LLM related events\n                df_llm \u003d df[df[\u0027topic\u0027].astype(str).str.contains(\u0027llm|external-model\u0027, case\u003dFalse, na\u003dFalse)].copy()\n                \n                if df_llm.empty:\n                    continue\n                \n                # Extract nested JSON fields. \n                # DSS flattens JSON by default, so \u0027usage.totalTokens\u0027 might already exist as a column.\n                # If not, we might need to parse the \u0027data\u0027 column if it came in as a string.\n                \n                # Normalizing column names based on standard audit log flattening\n                # Adjust these keys if your specific log version structure differs\n                cols \u003d df_llm.columns\n                \n                # Helper to safely get column data\n                def get_col(candidates, default\u003dNone):\n                    for c in candidates:\n                        if c in cols: return df_llm[c]\n                    return default\n\n                # Extract Cost\n                cost_col \u003d get_col([\u0027data.usage.estimatedCost\u0027, \u0027usage.estimatedCost\u0027])\n                costs \u003d cost_col if cost_col is not None else 0.0\n                \n                # Extract Tokens\n                tokens_col \u003d get_col([\u0027data.usage.totalTokens\u0027, \u0027usage.totalTokens\u0027])\n                tokens \u003d tokens_col if tokens_col is not None else 0\n                \n                # Extract Agent Name\n                # Try context first, then details\n                agent_col \u003d get_col([\u0027data.context.agentName\u0027, \u0027context.agentName\u0027, \n                                     \u0027data.details.agentName\u0027, \u0027details.agentName\u0027,\n                                     \u0027data.context.agentId\u0027])\n                agents \u003d agent_col.fillna(\"Direct/Unknown\") if agent_col is not None else \"Unknown\"\n\n                # Extract Model\n                model_col \u003d get_col([\u0027data.details.llmId\u0027, \u0027details.llmId\u0027, \n                                     \u0027data.target.llmId\u0027, \u0027target.llmId\u0027])\n                models \u003d model_col.fillna(\"N/A\") if model_col is not None else \"N/A\"\n\n                # Aggregate locally for this chunk\n                df_llm[\u0027extracted_cost\u0027] \u003d pd.to_numeric(costs, errors\u003d\u0027coerce\u0027).fillna(0)\n                df_llm[\u0027extracted_tokens\u0027] \u003d pd.to_numeric(tokens, errors\u003d\u0027coerce\u0027).fillna(0)\n                df_llm[\u0027extracted_agent\u0027] \u003d agents\n                df_llm[\u0027extracted_model\u0027] \u003d models\n                \n                chunk_group \u003d df_llm.groupby([\u0027extracted_agent\u0027, \u0027extracted_model\u0027]).agg({\n                    \u0027extracted_cost\u0027: \u0027sum\u0027,\n                    \u0027extracted_tokens\u0027: \u0027sum\u0027,\n                    \u0027topic\u0027: \u0027count\u0027 # Call count\n                }).reset_index()\n                \n                # Merge into main stats\n                for _, row in chunk_group.iterrows():\n                    key \u003d (row[\u0027extracted_agent\u0027], row[\u0027extracted_model\u0027])\n                    if key not in stats:\n                        stats[key] \u003d {\u0027cost\u0027: 0.0, \u0027tokens\u0027: 0, \u0027calls\u0027: 0}\n                    stats[key][\u0027cost\u0027] +\u003d row[\u0027extracted_cost\u0027]\n                    stats[key][\u0027tokens +\u003d row[\u0027extracted_tokens\u0027]\n                    stats[key][\u0027calls\u0027] +\u003d row[\u0027topic\u0027]\n\n        # Format final output\n        results \u003d []\n        for (agent, model), metrics in stats.items():\n            results.append({\n                \"Agent Name\": agent,\n                \"LLM Model\": model,\n                \"Total Cost ($)\": round(metrics[\u0027cost\u0027], 4),\n                \"Total Tokens\": int(metrics[\u0027tokens\u0027]),\n                \"Call Count\": int(metrics[\u0027calls\u0027])\n            })\n            \n        return pd.DataFrame(results).sort_values(\"Total Cost ($)\", ascending\u003dFalse)\n\n    finally:\n        print(\"4. Cleaning up temporary artifacts...\")\n        try:\n            dataset.delete()\n            conn.delete()\n        except:\n            pass\n\n# --- Execution ---\n# Update path if your previous \u0027ls\u0027 command showed a different root\nLOG_PATH \u003d \"/data/dataiku/dss_data/run/audit\" \n\ndf_report \u003d analyze_agent_costs_via_connection(LOG_PATH)\n\nif df_report is not None and not df_report.empty:\n    print(\"\\n--- Agent Cost \u0026 Utilization Report ---\")\n    print(df_report.to_string(index\u003dFalse))\nelse:\n    print(\"\\nNo utilization found or access failed.\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}