{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.21",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "creator": "demo",
    "modifiedBy": "demo",
    "createdOn": 1756999673948,
    "customFields": {},
    "tags": []
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name \u003d \"Dataiku_Hive_Project_Migration\""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datetime import datetime\n\nimport dataiku\nimport pandas as pd\nimport pprint\n\nfrom io import StringIO\nfrom IPython.display import FileLink\n\n# Connect to the DSS instance\nclient \u003d dataiku.api_client()\nproject \u003d client.get_default_project()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "existing_datasets \u003d [ds[\"name\"] for ds in project.list_datasets()]\n\nif dataset_name in existing_datasets:\n    print(f\"✅ Dataset \u0027{dataset_name}\u0027 already exists.\")\nelse:\n    print(f\"⚠️ Dataset \u0027{dataset_name}\u0027 not found. Creating it now...\")\n    builder \u003d project.new_managed_dataset(dataset_name) \\\n                     .with_store_into(\"filesystem_managed\") \\\n                     .with_format(\"csv\")\n\n    builder.create()\n\n    print(f\"✅ Dataset \u0027{dataset_name}\u0027 has been created.\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare a list to store results\nproject_hive_counts \u003d []\n\n# Iterate over all projects\nprojects \u003d client.list_project_keys()\nfor project_key in projects:\n    project \u003d client.get_project(project_key)\n    \n    # Get the list of recipes in the project\n    recipes \u003d project.list_recipes()\n    \n    # Count the number of Hive recipes\n    hive_count \u003d sum(1 for recipe in recipes if recipe[\u0027type\u0027] \u003d\u003d \u0027Hive\u0027)\n\n    # Append the result to the list\n    project_hive_counts.append({\u0027project_key\u0027: project_key, \u0027hive_recipe_count\u0027: hive_count})\n\n# Create a Pandas DataFrame with the results\ndf \u003d pd.DataFrame(project_hive_counts)\n# df.sort_values(by\u003d\"hive_recipe_count\", ascending\u003dFalse)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare a list to store results\nproject_info \u003d []\n\n# Iterate over all projects\nprojects \u003d client.list_project_keys()\nfor project_key in projects:\n    try:\n        project \u003d client.get_project(project_key)\n        \n        # Get project metadata to extract owner information\n        project_metadata \u003d project.get_metadata()\n        project_summary \u003d project.get_summary()\n        last_modified_timestamp \u003d project_summary.get(\u0027versionTag\u0027, None).get(\u0027lastModifiedOn\u0027, None)\n        if last_modified_timestamp:\n            last_modified_date \u003d datetime.fromtimestamp(last_modified_timestamp / 1000).strftime(\u0027%Y-%m-%d %H:%M:%S\u0027)\n        else:\n            last_modified_date \u003d \u0027unknown_date\u0027\n        \n        # Get the list of recipes in the project\n        recipes \u003d project.list_recipes()\n        \n        # Count the number of Hive recipes\n        hive_recipes \u003d [recipe for recipe in recipes if recipe[\u0027type\u0027].lower() \u003d\u003d \u0027hive\u0027]\n        hive_count \u003d len(hive_recipes)\n        \n        dataset_types \u003d set()\n        \n        for d in  project.list_datasets():\n            dataset_types.add(d.get(\u0027type\u0027))\n            \n        recipe_types \u003d set()\n        for r in  project.list_recipes():\n            recipe_types.add(r.get(\u0027type\u0027))\n        \n        # Append the result to the list\n        project_info.append({\n            \u0027PROJECT_KEY\u0027: project_key, \n            \u0027NUMBER_OF_HIVE_RECIPES_USED_IN_THIS_PROJECT\u0027: hive_count,\n            \u0027Dataset_types_used_in_project\u0027: dataset_types,\n            \u0027Owner\u0027: project.get_permissions().get(\u0027owner\u0027, \u0027unknown owner\u0027),\n            \u0027DATE_PROJECT_WAS_LAST_MODIFIED\u0027: last_modified_date,\n            \u0027Recipe_types_used_in_project\u0027: recipe_types,\n        })\n    except Exception as e:\n        print(f\"Error processing project {project_key}: {str(e)}\")\n\n# Create a Pandas DataFrame with the results\ndf \u003d pd.DataFrame(project_info)\ndf"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(df)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pk \u003d dataiku.default_project_key()\nprint(pk)\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "output_dataset \u003d dataiku.Dataset(dataset_name, project_key\u003dpk)\n# output_dataset.write_with_schema(df)\noutput_dataset.write_dataframe(df)\n#output_dataset.write_dataframe(df, drop_and_create\u003dTrue)\n"
      ],
      "outputs": []
    }
  ]
}