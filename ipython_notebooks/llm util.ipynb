{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.21",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "createdOn": 1764095416956,
    "customFields": {},
    "creator": "demo",
    "tags": [],
    "modifiedBy": "demo"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nimport pandas as pd\nimport time\nimport json\n\ndef analyze_agent_costs_via_connection(audit_path\u003d\"/data/dataiku/dss_data/run/audit\"):\n    client \u003d dataiku.api_client()\n    project \u003d client.get_default_project()\n    \n    # Unique names for temp objects\n    ts \u003d int(time.time())\n    conn_name \u003d f\"tmp_audit_conn_{ts}\"\n    ds_name \u003d f\"tmp_audit_logs_{ts}\"\n    \n    dataset \u003d None\n    conn \u003d None\n\n    print(f\"1. Creating temporary connection to: {audit_path}\")\n    try:\n        # Create a Filesystem connection rooted at the audit log path\n        conn \u003d client.create_connection(conn_name, \"Filesystem\", {\n            \"root\": audit_path\n        })\n    except Exception as e:\n        print(f\"Error creating connection: {e}\")\n        return None\n\n    try:\n        print(f\"2. Creating temporary dataset: {ds_name}\")\n        dataset \u003d project.create_dataset(ds_name, \"Filesystem\", params\u003d{\n            \"connection\": conn_name,\n            \"path\": \"/\"\n        }, formatType\u003d\"json\")\n        \n        # 2.1 Configure format (One JSON object per line)\n        settings \u003d dataset.get_settings()\n        settings.get_raw()[\"formatParams\"] \u003d {\n            \"style\": \"no_array\",\n            \"charset\": \"utf8\",\n            \"ignoreBadRecords\": True\n        }\n        settings.save()\n\n        # --- THE FIX ---\n        # 2.5 Force Schema Autodetection\n        # This scans the file content to find columns like \u0027topic\u0027, \u0027data\u0027, \u0027timestamp\u0027\n        print(\"   - Detecting schema from log files...\")\n        settings \u003d dataset.autodetect_settings()\n        settings.save() # Save the detected schema\n        print(f\"   - Schema detected: {[c[\u0027name\u0027] for c in settings.get_raw().get(\u0027schema\u0027, {}).get(\u0027columns\u0027, [])]}\")\n        # ----------------\n\n        print(\"3. Reading and aggregating data...\")\n        \n        stats \u003d {}\n        dku_ds \u003d dataiku.Dataset(ds_name)\n        \n        # Iterate in chunks\n        for df in dku_ds.iter_dataframes(chunksize\u003d10000):\n            if \u0027topic\u0027 in df.columns:\n                # Filter for LLM topics\n                df_llm \u003d df[df[\u0027topic\u0027].astype(str).str.contains(\u0027llm|external-model\u0027, case\u003dFalse, na\u003dFalse)].copy()\n                \n                if df_llm.empty:\n                    continue\n                \n                # Extract nested JSON fields safely\n                cols \u003d df_llm.columns\n                \n                def get_col(candidates, default\u003dNone):\n                    for c in candidates:\n                        if c in cols: return df_llm[c]\n                    return default\n\n                # Extract Cost, Tokens, Agent info\n                # Note: DSS flattens JSON, so \u0027data.usage.estimatedCost\u0027 is the column name\n                cost_col \u003d get_col([\u0027data.usage.estimatedCost\u0027, \u0027usage.estimatedCost\u0027])\n                costs \u003d cost_col if cost_col is not None else 0.0\n                \n                tokens_col \u003d get_col([\u0027data.usage.totalTokens\u0027, \u0027usage.totalTokens\u0027])\n                tokens \u003d tokens_col if tokens_col is not None else 0\n                \n                agent_col \u003d get_col([\u0027data.context.agentName\u0027, \u0027context.agentName\u0027, \n                                     \u0027data.details.agentName\u0027, \u0027details.agentName\u0027,\n                                     \u0027data.context.agentId\u0027])\n                agents \u003d agent_col.fillna(\"Direct/Unknown\") if agent_col is not None else \"Unknown\"\n\n                model_col \u003d get_col([\u0027data.details.llmId\u0027, \u0027details.llmId\u0027, \n                                     \u0027data.target.llmId\u0027, \u0027target.llmId\u0027])\n                models \u003d model_col.fillna(\"N/A\") if model_col is not None else \"N/A\"\n\n                # Aggregate locally\n                df_llm[\u0027extracted_cost\u0027] \u003d pd.to_numeric(costs, errors\u003d\u0027coerce\u0027).fillna(0)\n                df_llm[\u0027extracted_tokens\u0027] \u003d pd.to_numeric(tokens, errors\u003d\u0027coerce\u0027).fillna(0)\n                df_llm[\u0027extracted_agent\u0027] \u003d agents\n                df_llm[\u0027extracted_model\u0027] \u003d models\n                \n                chunk_group \u003d df_llm.groupby([\u0027extracted_agent\u0027, \u0027extracted_model\u0027]).agg({\n                    \u0027extracted_cost\u0027: \u0027sum\u0027,\n                    \u0027extracted_tokens\u0027: \u0027sum\u0027,\n                    \u0027topic\u0027: \u0027count\u0027\n                }).reset_index()\n                \n                for _, row in chunk_group.iterrows():\n                    key \u003d (row[\u0027extracted_agent\u0027], row[\u0027extracted_model\u0027])\n                    if key not in stats:\n                        stats[key] \u003d {\u0027cost\u0027: 0.0, \u0027tokens\u0027: 0, \u0027calls\u0027: 0}\n                    stats[key][\u0027cost\u0027] +\u003d row[\u0027extracted_cost\u0027]\n                    stats[key][\u0027tokens\u0027] +\u003d row[\u0027extracted_tokens\u0027]\n                    stats[key][\u0027calls\u0027] +\u003d row[\u0027topic\u0027]\n\n        results \u003d []\n        for (agent, model), metrics in stats.items():\n            results.append({\n                \"Agent Name\": agent,\n                \"LLM Model\": model,\n                \"Total Cost ($)\": round(metrics[\u0027cost\u0027], 4),\n                \"Total Tokens\": int(metrics[\u0027tokens\u0027]),\n                \"Call Count\": int(metrics[\u0027calls\u0027])\n            })\n            \n        if not results:\n            return pd.DataFrame(columns\u003d[\"Message\"]) \n            \n        return pd.DataFrame(results).sort_values(\"Total Cost ($)\", ascending\u003dFalse)\n\n    except Exception as e:\n        print(f\"\\nCRITICAL ERROR: {e}\")\n        return None\n\n    finally:\n        print(\"4. Cleaning up temporary artifacts...\")\n        if dataset:\n            try: dataset.delete()\n            except: pass\n        if conn:\n            try: conn.delete()\n            except: pass\n\n# --- Execution ---\nLOG_PATH \u003d \"/data/dataiku/dss_data/run/audit\" \ndf_report \u003d analyze_agent_costs_via_connection(LOG_PATH)\n\nif df_report is not None and not df_report.empty:\n    print(\"\\n--- Agent Cost \u0026 Utilization Report ---\")\n    print(df_report.to_string(index\u003dFalse))\nelse:\n    print(\"\\nNo utilization data found.\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}